<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog posts on It&#39;s a banana? No it&#39;s a blog.</title>
    <link>https://itsabanana.dev/posts/</link>
    <description>Recent content in Blog posts on It&#39;s a banana? No it&#39;s a blog.</description>
    <generator>Hugo</generator>
    <language>en-GB</language>
    <lastBuildDate>Wed, 27 Aug 2025 10:39:38 +0100</lastBuildDate>
    <atom:link href="https://itsabanana.dev/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Local LLM models: Part1 - getting started</title>
      <link>https://itsabanana.dev/posts/local_llm_hosting-part1/</link>
      <pubDate>Wed, 27 Aug 2025 10:39:38 +0100</pubDate>
      <guid>https://itsabanana.dev/posts/local_llm_hosting-part1/</guid>
      <description>&lt;p&gt;This will be a series of posts about running LLMs locally.&#xA;By the end we should have our own web based chat interface which can call tools&#xA;we have defined - e.g. to use a web search API, retrieve pages and feed them back&#xA;into the model.&lt;/p&gt;&#xA;&lt;p&gt;This post covers the first steps:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Setting up an inference server with &lt;a href=&#34;https://github.com/ggml-org/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;,&lt;/li&gt;&#xA;&lt;li&gt;Downloading a model - we&amp;rsquo;ll use &lt;a href=&#34;https://github.com/openai/gpt-oss&#34;&gt;gpt-oss&lt;/a&gt; from OpenAI.&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
